{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Embedding, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'Model_output/LSTM'\n",
    "\n",
    "epochs = 4\n",
    "batch_size = 128\n",
    "\n",
    "n_dim = 64\n",
    "n_unique_words = 5000\n",
    "n_words_to_skip = 50\n",
    "\n",
    "max_review_length = 10\n",
    "drop_embed = 0.2\n",
    "pad_type = 'pre'\n",
    "trunc_type = 'pre'\n",
    "\n",
    "n_rnn = 256\n",
    "drop_rnn = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can do the same thing with pandas\n",
    "raw_data = pd.read_json('/Users/karolinabogacka/Desktop/inzynierka/data.json',\n",
    "                        lines=True,\n",
    "                        orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>an,title</td>\n",
       "      <td>{'labels': ['N/A'], 'note': ''}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1529677325000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>STARKY0020180125ee1o0001i,Raila has crossed th...</td>\n",
       "      <td>{'labels': ['N/A'], 'note': ''}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1529758904000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>MTPW000020180409ee4900899,Political risk remai...</td>\n",
       "      <td>{'labels': ['N/A'], 'note': ''}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1529688087000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AFNWS00020171114edbe0018o,You Can Still Have Y...</td>\n",
       "      <td>{'labels': ['N/A'], 'note': ''}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1529587470000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>JMATH00020171219edcj0008f,Fuzzy Logic; Studies...</td>\n",
       "      <td>{'labels': ['N/A'], 'note': ''}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1529659936000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>DALYIG0020171214edce000jw,\"Nigeria Spends N43t...</td>\n",
       "      <td>{'labels': ['Not Distress'], 'note': ''}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1529646472000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14451</td>\n",
       "      <td>SNLMMDW020171214edcd0000b,Intrepid Mines selli...</td>\n",
       "      <td>{'labels': ['Not Distress'], 'note': ''}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1529670790000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14452</td>\n",
       "      <td>DJDN000020171211edcb001dd,Global Commodities R...</td>\n",
       "      <td>{'labels': ['Not Distress'], 'note': ''}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1529669474000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14453</td>\n",
       "      <td>BUSIDA0020180403ee4300003,\"Equity, Stanbic top...</td>\n",
       "      <td>{'labels': ['Not Distress'], 'note': ''}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1529601535000, 'last_updated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14454</td>\n",
       "      <td>ICROWDN020180226ee2q0008f,\"Solar Glass Market ...</td>\n",
       "      <td>{'labels': ['Not Distress'], 'note': ''}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'first_done_at': 1529589340000, 'last_updated...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14455 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  \\\n",
       "0                                               an,title   \n",
       "1      STARKY0020180125ee1o0001i,Raila has crossed th...   \n",
       "2      MTPW000020180409ee4900899,Political risk remai...   \n",
       "3      AFNWS00020171114edbe0018o,You Can Still Have Y...   \n",
       "4      JMATH00020171219edcj0008f,Fuzzy Logic; Studies...   \n",
       "...                                                  ...   \n",
       "14450  DALYIG0020171214edce000jw,\"Nigeria Spends N43t...   \n",
       "14451  SNLMMDW020171214edcd0000b,Intrepid Mines selli...   \n",
       "14452  DJDN000020171211edcb001dd,Global Commodities R...   \n",
       "14453  BUSIDA0020180403ee4300003,\"Equity, Stanbic top...   \n",
       "14454  ICROWDN020180226ee2q0008f,\"Solar Glass Market ...   \n",
       "\n",
       "                                     annotation  extras  \\\n",
       "0               {'labels': ['N/A'], 'note': ''}     NaN   \n",
       "1               {'labels': ['N/A'], 'note': ''}     NaN   \n",
       "2               {'labels': ['N/A'], 'note': ''}     NaN   \n",
       "3               {'labels': ['N/A'], 'note': ''}     NaN   \n",
       "4               {'labels': ['N/A'], 'note': ''}     NaN   \n",
       "...                                         ...     ...   \n",
       "14450  {'labels': ['Not Distress'], 'note': ''}     NaN   \n",
       "14451  {'labels': ['Not Distress'], 'note': ''}     NaN   \n",
       "14452  {'labels': ['Not Distress'], 'note': ''}     NaN   \n",
       "14453  {'labels': ['Not Distress'], 'note': ''}     NaN   \n",
       "14454  {'labels': ['Not Distress'], 'note': ''}     NaN   \n",
       "\n",
       "                                                metadata  \n",
       "0      {'first_done_at': 1529677325000, 'last_updated...  \n",
       "1      {'first_done_at': 1529758904000, 'last_updated...  \n",
       "2      {'first_done_at': 1529688087000, 'last_updated...  \n",
       "3      {'first_done_at': 1529587470000, 'last_updated...  \n",
       "4      {'first_done_at': 1529659936000, 'last_updated...  \n",
       "...                                                  ...  \n",
       "14450  {'first_done_at': 1529646472000, 'last_updated...  \n",
       "14451  {'first_done_at': 1529670790000, 'last_updated...  \n",
       "14452  {'first_done_at': 1529669474000, 'last_updated...  \n",
       "14453  {'first_done_at': 1529601535000, 'last_updated...  \n",
       "14454  {'first_done_at': 1529589340000, 'last_updated...  \n",
       "\n",
       "[14455 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2934], [1133, 65, 6172, 4, 6173], [263, 577, 193, 4, 143, 9146, 5, 9147, 3, 15]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def pad_dict_list(dict_list, padel):\n",
    "    lmax = 0\n",
    "    length = len(dict_list)\n",
    "    for lname in dict_list:\n",
    "        lmax = max(lmax, len(lname))\n",
    "    for i in range(0, length):\n",
    "        ll = len(dict_list[i])\n",
    "        if ll < lmax:\n",
    "            dict_list[i].append(padel)   \n",
    "    return dict_list\n",
    "\n",
    "titles = [re.split(\"^(.+?),\", title)[-1] for title in raw_data.content]\n",
    "titles = [title.lower() for title in titles]\n",
    "titles = [''.join([c for c in title if c not in punctuation]) for title in titles]\n",
    "all_text2 = ' '.join(titles)\n",
    "# create a list of words\n",
    "words = all_text2.split()\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)\n",
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "titles_int = []\n",
    "for review in titles:\n",
    "    r = [vocab_to_int[w] for w in review.split()]\n",
    "    titles_int.append(r)\n",
    "print (titles_int[0:3])\n",
    "\n",
    "labels = [md['labels'] for md in raw_data.annotation]\n",
    "\n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if \"Distress\" in label:\n",
    "        encoded_labels.append(1)\n",
    "    elif \"Not Distress\" in label: \n",
    "        encoded_labels.append(-1) \n",
    "    else: \n",
    "        encoded_labels.append(0)\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "\n",
    "#labels = pad_dict_list(labels, '')\n",
    "provider = pd.DataFrame({'title': titles, 'label': labels}, columns=['title', 'label'])\n",
    "encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "reviews_len = [len(x) for x in titles_int]\n",
    "pd.Series(reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(reviews_len).describe()\n",
    "\n",
    "titles_int = [ titles_int[i] for i, l in enumerate(reviews_len) if l>0 ]\n",
    "encoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l> 0 ]\n",
    "#train, test = train_test_split(provider, test_size=0.1)\n",
    "#trainX, trainY = train\n",
    "#testX, testY = test\n",
    "\n",
    "#trainX = pad_sequences(trainX, maxlen=100, value='')\n",
    "#testX = pad_sequences(testX, maxlen=100, value='')\n",
    "\n",
    "#trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0, 2934],\n",
       "       [   0,    0,    0, ..., 6172,    4, 6173],\n",
       "       [ 263,  577,  193, ..., 9147,    3,   15],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  366,    9,  261],\n",
       "       [   0,    0,    0, ...,    3, 9126, 1487],\n",
       "       [1061, 4919,    9, ...,  400,    6,    9]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_features(reviews_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features\n",
    "\n",
    "titles_int = pad_features(titles_int, 10)\n",
    "titles_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added cell tutorial keras\n",
    "\n",
    "((X_Train, Y_Train), (X_Test, Y_Test)) = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "len_feat = len(titles_int)\n",
    "train_x = np.array(titles_int[0:int(split_frac*len_feat)])\n",
    "train_y = np.array(encoded_labels[0:int(split_frac*len_feat)])\n",
    "remaining_x = titles_int[int(split_frac*len_feat):]\n",
    "remaining_y = encoded_labels[int(split_frac*len_feat):]\n",
    "valid_x = np.array(remaining_x[0:int(len(remaining_x)*0.5)])\n",
    "valid_y = np.array(remaining_y[0:int(len(remaining_y)*0.5)])\n",
    "test_x = np.array(remaining_x[int(len(remaining_x)*0.5):])\n",
    "test_y = np.array(remaining_y[int(len(remaining_y)*0.5):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 10])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,   113,   138,  1451,  1432, 15327,   727,   384],\n",
      "        [  375,   640,   609,    49,    86,    13,  4987,     2,    55,    80],\n",
      "        [    0,     0,     0, 15968,  1010,     6,  1207,     2,     4, 15969],\n",
      "        [ 4595,     6, 15565,  2036,   754,  1488,   928,  5918,    20,     8],\n",
      "        [13637,  8252,    34,    11,   258,     3,   144,    14,     8,   122],\n",
      "        [    0,     0,     0,     0,     0,  6810,    69,  2437,   712,    52],\n",
      "        [ 1691,   412,   153,   112,   800,  8499, 14363,     3,   447,  4486],\n",
      "        [    0,     0,     0,  1474,  3040,   172,     5,   103,     6,  9607],\n",
      "        [    0,     0,     0,     0,  4729,   715,   402,  1982,    16,  5462],\n",
      "        [ 3222,   102,  1286,   358,    54,    22,     2,    21,   417,     3],\n",
      "        [    0,     0,   425,  8898,  8023,    18,    56,   160,  1066,   296],\n",
      "        [ 1327,     1,   212,    23,   273,    13,  5481,    58,    13,   724],\n",
      "        [    0,     0,     0,  1193,    51,  3529, 14583,    16,   327,   147],\n",
      "        [    0,     0,   202,    80,   532,     2,   215,  2418,  7401,  3266],\n",
      "        [    0,     0,   113,   513,    47,   385,  1192,   118,  5698,    83],\n",
      "        [  120,  2699,  2591,   718,     7,   440,  3263,  3831,   469,     1],\n",
      "        [    0,  1810,   182,  1213,   347, 13057,   389,    32,    82,   517],\n",
      "        [    0,  1298,   175,     5,   212,     5,     4,  1058,     2,    44],\n",
      "        [  475,   535,     2,   410,  1591,   130,    36,  1112,   665,   501],\n",
      "        [15371,     9,    45,    21,  4783,   279,    62,  3596,    62,  1802],\n",
      "        [    0,     0,     0,  3319,    71,   528,   110,  2916,  3865,     9],\n",
      "        [    0,    91,   108,   405,  4644, 12445,     5,   822,   590,  5029],\n",
      "        [    0,     0,     0,     0,  1640,   295,   457,  1184,     7,   160],\n",
      "        [    0, 16288, 16289,     6,    59,   849,     7,   826,  5849,   225],\n",
      "        [  342,   730,   703, 10839,     3,   333,   219,     5,   254,  4383],\n",
      "        [    0,     0,     0,     0,     0,     4,  4939,  5432,  4425,  4426],\n",
      "        [ 4776,    46,   369,    17,  1269,     2,  1619,  2387,    12,   127],\n",
      "        [   50,     1,  1456,    53,  5948,   112,  4372,   330,     7,  1084],\n",
      "        [    0,     0,     0,     0,   412,   453,   230,     3,  1136,   184],\n",
      "        [    0,     0,     0,     0,     0, 15228, 15229,  2864,   944, 15230],\n",
      "        [ 4953,     6,  4954,  4051,   149,    17,  9523,  9524,    85,     3],\n",
      "        [ 2130,  2147,     2,   302,   397,  2165,    64,   627,  6725,   387],\n",
      "        [   27,  9713,     9,    45,    26,  6481,  6482,   881,  9714,  3060],\n",
      "        [ 6078,  1753,  1754,    34,    11,   602,     3,   144,    14,   203],\n",
      "        [  311,  1590,     7,  2051,   344,   797, 16129,    83,    26,   117],\n",
      "        [ 1509,    65,    79,  3908,     1,   733,   406,    69,    28,   108],\n",
      "        [    0,     0,     0,     0,   117, 11404,  5176,     4, 11405,   146],\n",
      "        [    0,  3225,  1597,   776,  5757,     1,  3440,    19,  7804,  1841],\n",
      "        [  942,   943,   779,   329,    19,  2719,  9810,     6,  9811,    25],\n",
      "        [  610,    82,     1,   438,  1094,    55,    10,    56,  1832,    75],\n",
      "        [   19,    23,   131, 12559,  3862,     2,  3536,  1593,     3,  1691],\n",
      "        [    0,     0,     0, 14195,  1666,  1492,   341,  1461,     3,    45],\n",
      "        [    0,     0,     0,    66,   488,    21,   436,     1,   139,   232],\n",
      "        [    0,     0,     0,  1501,   118,   870,  6748,   525,  5186,  6749],\n",
      "        [16940,  1720,  2825, 16941,  6584,   830,   432, 16942, 16943,     3],\n",
      "        [  128,   158,   109,  1133,   504,  3444,  2555,     3,    10,    24],\n",
      "        [    0,     0,   867,  3954,     3,  5905,  1656,   289,     2, 16563],\n",
      "        [ 3156,  3317,  4142,   323,   514,     6,  8087,  1414,     5,     4],\n",
      "        [    0,     0,     0,     0,  5403,   179,  1030,    13,   158,  1155],\n",
      "        [  960,     5,     4,   287,   456,   522,  9909,  9910,   349,     2]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([ 0,  0,  0,  0, -1,  1,  1,  1, -1, -1,  0, -1,  0, -1,  0,  1,  0,  0,\n",
      "         0, -1, -1, -1,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,  0,  0,\n",
      "         0,  0, -1,  0,  1, -1,  0,  0,  0,  0,  0, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#create Tensor Dataset\n",
    "train_data=TensorDataset(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n",
    "valid_data=TensorDataset(torch.FloatTensor(valid_x), torch.FloatTensor(valid_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(test_x), torch.FloatTensor(test_y))\n",
    "\n",
    "#dataloader\n",
    "batch_size=50\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    " \n",
    "class SentimentalLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):    \n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_size=output_size\n",
    "        self.n_layers=n_layers\n",
    "        self.hidden_dim=hidden_dim\n",
    "        \n",
    "        #Embedding and LSTM layers\n",
    "        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        #dropout layer\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        \n",
    "        #Linear and sigmoid layer\n",
    "        self.fc1=nn.Linear(hidden_dim, 64)\n",
    "        self.fc2=nn.Linear(64, 16)\n",
    "        self.fc3=nn.Linear(16,output_size)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size=x.size()\n",
    "        \n",
    "        #Embadding and LSTM output\n",
    "        embedd=self.embedding(x)\n",
    "        lstm_out, hidden=self.lstm(embedd, hidden)\n",
    "        \n",
    "        #stack up the lstm output\n",
    "        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        #dropout and fully connected layers\n",
    "        out=self.dropout(lstm_out)\n",
    "        out=self.fc1(out)\n",
    "        out=self.dropout(out)\n",
    "        out=self.fc2(out)\n",
    "        out=self.dropout(out)\n",
    "        out=self.fc3(out)\n",
    "        sig_out=self.sigmoid(out)\n",
    "        \n",
    "        sig_out=sig_out.view(batch_size, -1)\n",
    "        sig_out=sig_out[:, -1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize Hidden STATE\"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentalLSTM(\n",
      "  (embedding): Embedding(19159, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOutput:\\n SentimentalLSTM(   \\n(embedding): Embedding(74073, 400)   \\n(lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)   (dropout): Dropout(p=0.3)   \\n(fc): Linear(in_features=256, out_features=1, bias=True)   (sigmoid): Sigmoid() )\\n'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)\n",
    "\n",
    "'''\n",
    "Output:\n",
    " SentimentalLSTM(   \n",
    "(embedding): Embedding(74073, 400)   \n",
    "(lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)   (dropout): Dropout(p=0.3)   \n",
    "(fc): Linear(in_features=256, out_features=1, bias=True)   (sigmoid): Sigmoid() )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-9f1fbf7df8ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mval_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_h\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m             raise RuntimeError(\n\u001b[1;32m    191\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# check if CUDA is available\n",
    "#train_on_gpu = torch.cuda.is_available()\n",
    "# training params\n",
    "\n",
    "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "#if(train_on_gpu):\n",
    "#    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        #if(train_on_gpu):\n",
    "        #    inputs=inputs.cuda()\n",
    "        #    labels=labels.cuda()\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs.long(), h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()  \n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n",
      "Test loss: nan\n",
      "Test accuracy: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karolinabogacka/miniconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/karolinabogacka/miniconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate loss\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        # convert output probabilities to predicted class (0 or 1)\n",
    "        pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "    # -- stats! -- ##\n",
    "    # avg test loss\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "    # accuracy over all test data\n",
    "    test_acc = num_correct/len(test_loader.dataset)\n",
    "    print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'build_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-1d06a5d373f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMAX_VOCAB_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25_000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m titles_int.build_vocab(train_data, \n\u001b[0m\u001b[1;32m      4\u001b[0m                  \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_VOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.6B.100d\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'build_vocab'"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "titles_int.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentimentLSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-ff158057204d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#SentimentRNN(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentimentLSTM' is not defined"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)\n",
    "#SentimentRNN(\n",
    "#  (embedding): Embedding(74073, 400)\n",
    "#  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
    "#  (dropout): Dropout(p=0.3)\n",
    "#  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
    "#  (sig): Sigmoid()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-c723b68c163f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'This movie had the best acting and the dialogue was so good. I loved it.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;31m# good to use the length that was trained on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_review_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "test_review = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
    "seq_length=200 # good to use the length that was trained on\n",
    "predict(net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is tensor([[    0,     0,    12,  1390,   365,     7,   176,   125,    61,  2809],\n",
      "        [    0,     0,     0,  5551,    98,  1501,     1,   409,    28,  1141],\n",
      "        [    0,     0,     0,     0,     0, 16335,  1362,   528,  5370,  2087],\n",
      "        [    0,     0,     0,     0,  4767,  1563, 14645,   288,     6,   500],\n",
      "        [    0,  1765,  1505,    75,    58,    13,   649,   766,    26,  8458],\n",
      "        [    0,   324,   500,     5,  2432,     1,  5871,  1329,  5145, 13141],\n",
      "        [    0,     0,     0,  5562,  1298,  3703,  5561,     7,   240,    30],\n",
      "        [    0,     0,     0,     0,     0,   765,     1,   139,   118, 11746],\n",
      "        [    0,     0,     0,   769,   236,  2416,  1482,    13,  5014,  6459],\n",
      "        [    0,    24,    29,  1185,     1,  3747,   677,   446,   530,  3666],\n",
      "        [    0,    38,    36,    78,     3,    21,   198,    23,    43,   111],\n",
      "        [    0, 12157,   883,     1,   681,  3836,     7,  7642, 12158,   185],\n",
      "        [    0,  6962,    60,  6963,  6964,    16,     4,    15,   382,   146],\n",
      "        [13707,   898,   245,  1428,    63,   360,     1,  1608,   213,   986],\n",
      "        [    0,   881,  1949,   264,   155,    11,  9422,    25,    30,    22],\n",
      "        [  312,    21,  7581,    59,    68,  7785,    10,   112,   546,    77],\n",
      "        [    0,     0,     0,     0,     0,   787,  1497,     4,  1295,    52],\n",
      "        [    0,     0,  1355,   294,   136,    11,  8535,    25,    30,    22],\n",
      "        [   41,    51,   749,  8905,    17, 16355,     2,  5487,   829,     3],\n",
      "        [    0,   194,   181,   130,   703,  4183,    14,    63,   947,  5113],\n",
      "        [    0,     0,   108,   388,  2007,   892,  2704, 16369,    28,  1400],\n",
      "        [    0,     0,     0,   837,     1,   383,  3277,     7, 14409,  2551],\n",
      "        [    0,     0,   224,   450,     5,  4315,  7748,    10,  3086,   286],\n",
      "        [    0,     0,  4389,    80,   875,   370,     1, 13963,    42, 13964],\n",
      "        [    0,     0,   759,    53,     7,   661,   505,  9342,    74,   293],\n",
      "        [   68,   332,  1927,     2,  2626,  8216,  1805,  2042, 13536,   564],\n",
      "        [    0,     0,     0,     0,    52,  2993,  1414,    16, 10700,   713],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,   232,    20,  7330],\n",
      "        [  336,  3353, 14256,   654,  2842,    17,   465,  2960,    32,   112],\n",
      "        [    0,     0,     0,  4482,  6115,   240,   215,   576,     3,  5880],\n",
      "        [    0,     0,     0,     0,     0,    47,     2,    84,    83,   119],\n",
      "        [    0,     0,     0,  1499,   846,    11,  7195,    25,    30,    22],\n",
      "        [    0,   420,  1078,   239,     1,  6184,  2095,   402,   240,   700],\n",
      "        [    0,     0,     0,     0,     0,     0,  3615, 14647,   253,   474],\n",
      "        [    0,     0,     0,    50,   381,   221, 15502,     3,    23,   166],\n",
      "        [  131,   738,    74,  1273,  5123, 13800,    93,    18,  2046,   211],\n",
      "        [    0,     0,     0,     0,     0,   116,  7738,   236,  5716,    70],\n",
      "        [    0,     0,     0,     0,     0,   265,  3560,  1550,  8838,   756],\n",
      "        [    0,    39,  8665,  2778,  1222,    97,    16, 15124,   832,    88],\n",
      "        [    0,     0,     0,     0,     0,  3912,   903,     4,  3410,   493],\n",
      "        [    0,     0,  2693,     2,   505,   350,   711,     7, 14453,   444],\n",
      "        [    0,     0,     0,     0,   441,  1543,     1,  1444,     3,    41],\n",
      "        [ 1622,    34,    11,   258,     3,   144,    14,   203,     4,    94],\n",
      "        [    0,     0,     0,     0,     0,   497,  2204,  2578,   387,  1831],\n",
      "        [    0,     0,     0,     0, 13725, 13726,   145,     5,    21,  4376],\n",
      "        [ 2945,   942,   943,   779,   329,    19,    15,   133,  6106,    25],\n",
      "        [ 5148,  6676,    60,    11,   116,     7,   448,     2,   503,  2315],\n",
      "        [    0,  8092,  1197,  1099,  2670,   275,   492,  1300,     3,   479],\n",
      "        [    0,     0,     0,     0,     0,    39,   624,  5371,  2527,  5372],\n",
      "        [    0,   428,     1,   383, 13819,  3316,     3,  1539,   495,   505]])\n",
      "Shape of X and y are : torch.Size([50, 10]) torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "x, y = dataiter.next()\n",
    "x = x.type(torch.LongTensor)\n",
    "print ('X is', x)\n",
    "print ('Shape of X and y are :', x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer is  Embedding(19159, 400)\n",
      "Embedding layer weights  torch.Size([19159, 400])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "#vocab_size = len(words)\n",
    "#embedding_dim = 30\n",
    "embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "print ('Embedding layer is ', embeds)\n",
    "print ('Embedding layer weights ', embeds.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer output shape torch.Size([50, 10, 400])\n",
      "Embedding layer output  tensor([[[-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         [-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         [ 0.3653,  0.5619, -0.9994,  ...,  0.6369,  0.1369,  0.1119],\n",
      "         ...,\n",
      "         [ 1.0648, -1.2052,  1.4546,  ...,  0.0962, -1.9906,  0.1895],\n",
      "         [-2.1292, -1.4613,  0.5152,  ...,  0.3583, -0.4657,  0.5489],\n",
      "         [-0.4621, -0.1879, -0.4545,  ..., -1.4743,  2.0220,  0.2461]],\n",
      "\n",
      "        [[-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         [-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         [-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         ...,\n",
      "         [ 0.0809, -0.3786, -1.3089,  ...,  1.3704, -0.1527,  0.0671],\n",
      "         [-1.0816,  2.4431, -0.1672,  ..., -2.0787, -2.3367, -0.1005],\n",
      "         [-0.4470, -0.6543,  0.6490,  ..., -0.0703,  0.4985,  0.2829]],\n",
      "\n",
      "        [[-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         [-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         [-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         ...,\n",
      "         [-0.9981, -0.3494,  1.8361,  ..., -1.2707,  0.3598,  0.6281],\n",
      "         [-0.1761,  0.8206, -0.1965,  ...,  1.1209,  0.7626, -0.4039],\n",
      "         [ 0.4516, -1.0080, -0.7529,  ..., -0.5840,  0.6527, -0.0617]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         [ 0.9700,  0.7227, -0.0393,  ...,  0.0406, -1.5522, -0.2615],\n",
      "         [ 0.5891,  1.2287, -0.5993,  ..., -0.1258, -0.9122,  0.9956],\n",
      "         ...,\n",
      "         [-0.7960,  0.8332, -0.4266,  ..., -0.6284, -0.1238,  0.9662],\n",
      "         [-0.0030,  1.2154, -0.7797,  ...,  2.3130,  0.6965,  1.1510],\n",
      "         [-0.6689,  1.0754, -0.8443,  ...,  0.3118,  0.2846, -1.8276]],\n",
      "\n",
      "        [[-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         [-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         [-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         ...,\n",
      "         [-1.3932,  0.7280,  1.1274,  ..., -0.6243,  0.7225,  0.5151],\n",
      "         [-1.3294,  0.4070,  0.6780,  ...,  0.6028,  0.1722, -2.4901],\n",
      "         [-0.1266,  0.7613,  0.4362,  ..., -0.0408, -0.2702, -0.0567]],\n",
      "\n",
      "        [[-1.6654, -0.2606,  0.4355,  ...,  0.1795,  0.3170,  0.0574],\n",
      "         [-0.8341, -0.3405, -1.4894,  ...,  1.4492, -1.2146, -0.5630],\n",
      "         [-1.3259, -0.1757,  1.7873,  ..., -1.3754,  1.9431, -0.1186],\n",
      "         ...,\n",
      "         [-0.4256, -0.8830, -0.2608,  ..., -1.1136,  1.1499, -1.3740],\n",
      "         [-0.1712, -0.0799, -1.6676,  ...,  0.0118, -1.3813,  1.3830],\n",
      "         [-0.5302, -0.7935, -1.1144,  ..., -0.0097,  1.3952, -0.8831]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "embeds_out = embeds(x)\n",
    "print ('Embedding layer output shape', embeds_out.shape)\n",
    "print ('Embedding layer output ', embeds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer output shape torch.Size([50, 10, 512])\n",
      "LSTM layer output  tensor([[[ 0.1556,  0.0139, -0.1684,  ..., -0.0619, -0.0242, -0.0169],\n",
      "         [ 0.1835,  0.0222, -0.2512,  ..., -0.0943, -0.0280, -0.0153],\n",
      "         [ 0.1013,  0.0030,  0.1327,  ..., -0.1362,  0.0913,  0.0555],\n",
      "         ...,\n",
      "         [-0.1016, -0.0278, -0.0626,  ..., -0.1820,  0.1754, -0.1039],\n",
      "         [-0.0754,  0.0145,  0.0037,  ..., -0.0247,  0.1213,  0.1113],\n",
      "         [ 0.0354,  0.0433, -0.1399,  ..., -0.1688, -0.0039, -0.0324]],\n",
      "\n",
      "        [[ 0.1556,  0.0139, -0.1684,  ..., -0.0619, -0.0242, -0.0169],\n",
      "         [ 0.1835,  0.0222, -0.2512,  ..., -0.0943, -0.0280, -0.0153],\n",
      "         [ 0.1813,  0.0254, -0.2857,  ..., -0.1106, -0.0239, -0.0100],\n",
      "         ...,\n",
      "         [ 0.1590, -0.1168,  0.0253,  ..., -0.1588,  0.0022, -0.0900],\n",
      "         [ 0.0304, -0.0140,  0.1153,  ..., -0.0862,  0.1174, -0.0489],\n",
      "         [ 0.1257, -0.0842, -0.1120,  ..., -0.0057,  0.0818,  0.1156]],\n",
      "\n",
      "        [[ 0.1556,  0.0139, -0.1684,  ..., -0.0619, -0.0242, -0.0169],\n",
      "         [ 0.1835,  0.0222, -0.2512,  ..., -0.0943, -0.0280, -0.0153],\n",
      "         [ 0.1813,  0.0254, -0.2857,  ..., -0.1106, -0.0239, -0.0100],\n",
      "         ...,\n",
      "         [ 0.0398, -0.0155, -0.0395,  ...,  0.0473,  0.0915,  0.1815],\n",
      "         [ 0.0234, -0.0084, -0.0141,  ..., -0.0133,  0.1226, -0.0259],\n",
      "         [-0.1205,  0.0206,  0.1215,  ...,  0.2101,  0.0685, -0.1322]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1556,  0.0139, -0.1684,  ..., -0.0619, -0.0242, -0.0169],\n",
      "         [ 0.0823,  0.1847, -0.1102,  ..., -0.0695,  0.0954,  0.0312],\n",
      "         [-0.0560,  0.1792, -0.0133,  ...,  0.0952,  0.0491,  0.0214],\n",
      "         ...,\n",
      "         [ 0.0098,  0.2336,  0.2530,  ..., -0.1488,  0.0728,  0.0356],\n",
      "         [ 0.0856,  0.1759,  0.2750,  ..., -0.1123,  0.1683, -0.1600],\n",
      "         [ 0.1401,  0.0243, -0.0137,  ..., -0.2838, -0.0556,  0.0530]],\n",
      "\n",
      "        [[ 0.1556,  0.0139, -0.1684,  ..., -0.0619, -0.0242, -0.0169],\n",
      "         [ 0.1835,  0.0222, -0.2512,  ..., -0.0943, -0.0280, -0.0153],\n",
      "         [ 0.1813,  0.0254, -0.2857,  ..., -0.1106, -0.0239, -0.0100],\n",
      "         ...,\n",
      "         [-0.1085, -0.1680,  0.1475,  ..., -0.0448, -0.0102,  0.0599],\n",
      "         [ 0.1262, -0.2381,  0.0276,  ..., -0.0179,  0.1355,  0.0755],\n",
      "         [-0.0597, -0.0031, -0.0200,  ..., -0.1206, -0.0953,  0.1489]],\n",
      "\n",
      "        [[ 0.1556,  0.0139, -0.1684,  ..., -0.0619, -0.0242, -0.0169],\n",
      "         [ 0.0665,  0.0746, -0.1386,  ..., -0.0408,  0.1306, -0.0431],\n",
      "         [ 0.2045, -0.0592, -0.2138,  ..., -0.1064,  0.0587,  0.0341],\n",
      "         ...,\n",
      "         [-0.0758, -0.1643,  0.0382,  ..., -0.0283,  0.0255,  0.1100],\n",
      "         [-0.0041,  0.0160,  0.0967,  ...,  0.0809,  0.0655, -0.2075],\n",
      "         [ 0.0674,  0.0857, -0.0423,  ...,  0.1383,  0.0307, -0.0450]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# initializing the hidden state to 0\n",
    "hidden=None\n",
    "lstm = nn.LSTM(input_size=embedding_dim, hidden_size=512, num_layers=1, batch_first=True)\n",
    "lstm_out, h = lstm(embeds_out, hidden)\n",
    "print ('LSTM layer output shape', lstm_out.shape)\n",
    "print ('LSTM layer output ', lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC layer output shape torch.Size([500, 1])\n",
      "FC layer output  tensor([[-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.2018e-01],\n",
      "        [-3.7867e-02],\n",
      "        [-7.1729e-02],\n",
      "        [-3.6254e-02],\n",
      "        [-1.1194e-01],\n",
      "        [-1.2713e-01],\n",
      "        [-1.6259e-01],\n",
      "        [-2.5879e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.9117e-01],\n",
      "        [-1.0549e-01],\n",
      "        [-3.5253e-02],\n",
      "        [-8.9207e-02],\n",
      "        [-8.1496e-02],\n",
      "        [-8.5122e-03],\n",
      "        [ 3.7097e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [-9.6950e-02],\n",
      "        [-1.5696e-01],\n",
      "        [-8.6435e-03],\n",
      "        [-5.9798e-02],\n",
      "        [-8.3904e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.3486e-01],\n",
      "        [-1.4522e-01],\n",
      "        [-3.4458e-02],\n",
      "        [-2.4367e-02],\n",
      "        [-7.4449e-02],\n",
      "        [-2.3766e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-6.5617e-02],\n",
      "        [ 2.6501e-02],\n",
      "        [-1.9836e-02],\n",
      "        [ 5.3719e-03],\n",
      "        [ 2.8795e-03],\n",
      "        [-2.3769e-02],\n",
      "        [ 9.4393e-02],\n",
      "        [ 3.7366e-02],\n",
      "        [-1.6652e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-9.5405e-02],\n",
      "        [-6.4589e-02],\n",
      "        [ 1.0661e-02],\n",
      "        [-8.0089e-02],\n",
      "        [-9.5349e-02],\n",
      "        [-6.7418e-02],\n",
      "        [-3.9795e-02],\n",
      "        [ 1.4359e-02],\n",
      "        [ 1.2562e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.2722e-01],\n",
      "        [-7.9628e-02],\n",
      "        [-7.7305e-02],\n",
      "        [-1.3820e-01],\n",
      "        [-2.3795e-02],\n",
      "        [-4.3135e-02],\n",
      "        [-2.6110e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [-1.5956e-01],\n",
      "        [-1.9514e-01],\n",
      "        [ 5.8233e-03],\n",
      "        [ 2.3824e-02],\n",
      "        [-2.3075e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.1169e-01],\n",
      "        [-1.0377e-01],\n",
      "        [-7.3355e-02],\n",
      "        [-1.4106e-02],\n",
      "        [-1.8411e-02],\n",
      "        [-3.3605e-02],\n",
      "        [ 8.0996e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.8980e-01],\n",
      "        [-2.0394e-01],\n",
      "        [-3.9831e-02],\n",
      "        [-6.1987e-02],\n",
      "        [-1.1148e-01],\n",
      "        [-2.6802e-02],\n",
      "        [ 9.7003e-02],\n",
      "        [ 2.5068e-02],\n",
      "        [ 8.3142e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.0005e-01],\n",
      "        [-1.6729e-01],\n",
      "        [-2.5763e-01],\n",
      "        [-1.3739e-01],\n",
      "        [-6.2008e-02],\n",
      "        [-1.1097e-01],\n",
      "        [-1.0788e-01],\n",
      "        [-7.6381e-02],\n",
      "        [-2.1182e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.4170e-01],\n",
      "        [ 6.1766e-02],\n",
      "        [-5.3363e-02],\n",
      "        [-8.3917e-02],\n",
      "        [-8.4069e-02],\n",
      "        [-5.6773e-03],\n",
      "        [ 1.0452e-01],\n",
      "        [-1.1075e-02],\n",
      "        [-4.7096e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-8.1659e-02],\n",
      "        [-1.4432e-01],\n",
      "        [-1.3502e-01],\n",
      "        [-9.4080e-02],\n",
      "        [-3.0024e-02],\n",
      "        [-7.7617e-02],\n",
      "        [-4.8965e-02],\n",
      "        [-5.2689e-02],\n",
      "        [ 6.6030e-02],\n",
      "        [-1.1666e-01],\n",
      "        [-2.2958e-01],\n",
      "        [-1.5693e-01],\n",
      "        [-6.0548e-02],\n",
      "        [-1.2284e-01],\n",
      "        [-1.1351e-01],\n",
      "        [-1.0737e-01],\n",
      "        [-1.4831e-01],\n",
      "        [-2.0169e-02],\n",
      "        [-4.1993e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-9.1949e-02],\n",
      "        [-1.0443e-01],\n",
      "        [-7.6441e-02],\n",
      "        [ 9.5309e-02],\n",
      "        [ 4.0945e-02],\n",
      "        [-1.2235e-02],\n",
      "        [ 3.3923e-02],\n",
      "        [ 1.2974e-02],\n",
      "        [ 4.7705e-02],\n",
      "        [ 1.0648e-02],\n",
      "        [-2.1601e-02],\n",
      "        [ 6.5927e-02],\n",
      "        [-6.1255e-02],\n",
      "        [-1.0509e-01],\n",
      "        [-6.3101e-02],\n",
      "        [ 2.0875e-02],\n",
      "        [-8.7539e-02],\n",
      "        [-2.2276e-02],\n",
      "        [-3.6634e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [-1.6651e-01],\n",
      "        [-1.3473e-01],\n",
      "        [-1.3804e-01],\n",
      "        [-2.6930e-01],\n",
      "        [-1.6141e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.3700e-01],\n",
      "        [-1.6757e-01],\n",
      "        [-1.1776e-01],\n",
      "        [-5.2070e-02],\n",
      "        [-8.0376e-02],\n",
      "        [ 2.3962e-02],\n",
      "        [ 1.3778e-02],\n",
      "        [ 5.2460e-02],\n",
      "        [-7.1403e-02],\n",
      "        [-7.2470e-02],\n",
      "        [-4.3558e-02],\n",
      "        [ 6.2548e-02],\n",
      "        [ 3.1874e-02],\n",
      "        [ 5.4616e-02],\n",
      "        [ 2.1376e-02],\n",
      "        [ 1.0822e-03],\n",
      "        [-8.6576e-03],\n",
      "        [ 4.7674e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2251e-01],\n",
      "        [-6.2171e-02],\n",
      "        [-7.3188e-02],\n",
      "        [-3.1296e-03],\n",
      "        [-4.1167e-02],\n",
      "        [-1.7611e-01],\n",
      "        [-1.8514e-01],\n",
      "        [-1.0225e-01],\n",
      "        [-1.1981e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.9002e-02],\n",
      "        [-8.7215e-02],\n",
      "        [-9.9764e-02],\n",
      "        [-5.3626e-02],\n",
      "        [-1.2161e-01],\n",
      "        [-4.6575e-02],\n",
      "        [-1.6161e-02],\n",
      "        [ 5.4965e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-6.4754e-02],\n",
      "        [-1.1038e-01],\n",
      "        [-1.6972e-01],\n",
      "        [ 4.7829e-02],\n",
      "        [ 3.6476e-02],\n",
      "        [-1.0471e-02],\n",
      "        [-4.2990e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.0208e-01],\n",
      "        [-7.6167e-02],\n",
      "        [ 1.1963e-02],\n",
      "        [-8.6591e-02],\n",
      "        [-1.0952e-01],\n",
      "        [ 2.7868e-02],\n",
      "        [ 7.6366e-02],\n",
      "        [-1.3140e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.0460e-01],\n",
      "        [-1.8524e-01],\n",
      "        [-1.3807e-01],\n",
      "        [-2.3405e-01],\n",
      "        [-2.0581e-01],\n",
      "        [-1.3911e-01],\n",
      "        [-1.8923e-01],\n",
      "        [-1.8223e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-2.7459e-02],\n",
      "        [-7.9605e-03],\n",
      "        [ 1.0434e-02],\n",
      "        [-2.6419e-02],\n",
      "        [-1.6814e-02],\n",
      "        [-5.0628e-02],\n",
      "        [ 8.6703e-03],\n",
      "        [-3.8225e-02],\n",
      "        [-1.1040e-01],\n",
      "        [-8.3029e-03],\n",
      "        [-1.7215e-01],\n",
      "        [-6.9306e-02],\n",
      "        [-1.3510e-02],\n",
      "        [-4.5804e-02],\n",
      "        [-1.3184e-02],\n",
      "        [-1.9078e-02],\n",
      "        [-3.9928e-02],\n",
      "        [ 5.6695e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.4303e-01],\n",
      "        [-9.1161e-02],\n",
      "        [-1.3868e-01],\n",
      "        [-3.3601e-02],\n",
      "        [-3.3732e-02],\n",
      "        [-1.3529e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [-1.7496e-01],\n",
      "        [-1.7817e-01],\n",
      "        [-2.5241e-01],\n",
      "        [-1.8726e-01],\n",
      "        [-2.3545e-01],\n",
      "        [ 3.7765e-02],\n",
      "        [-1.3373e-02],\n",
      "        [-1.8763e-01],\n",
      "        [-6.2156e-02],\n",
      "        [-5.3748e-02],\n",
      "        [-5.1827e-02],\n",
      "        [-2.2856e-03],\n",
      "        [-1.8696e-02],\n",
      "        [ 3.4198e-02],\n",
      "        [-1.1450e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.3092e-01],\n",
      "        [-6.5790e-02],\n",
      "        [-6.8153e-02],\n",
      "        [-9.8098e-02],\n",
      "        [-6.6405e-02],\n",
      "        [-2.5593e-02],\n",
      "        [-6.9031e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [-2.6967e-01],\n",
      "        [-1.4245e-01],\n",
      "        [-1.5055e-01],\n",
      "        [-1.4772e-01],\n",
      "        [-1.5972e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.1000e-01],\n",
      "        [-1.3846e-01],\n",
      "        [-1.0008e-01],\n",
      "        [ 6.0291e-02],\n",
      "        [ 8.1487e-02],\n",
      "        [ 6.0546e-02],\n",
      "        [ 6.9396e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-2.4054e-03],\n",
      "        [-1.2606e-02],\n",
      "        [ 1.3343e-02],\n",
      "        [-7.9604e-02],\n",
      "        [-1.6808e-02],\n",
      "        [-4.5051e-02],\n",
      "        [-2.0982e-02],\n",
      "        [-2.7811e-02],\n",
      "        [-7.8217e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [-1.7496e-01],\n",
      "        [-7.2889e-02],\n",
      "        [ 7.9842e-03],\n",
      "        [-3.0755e-02],\n",
      "        [-1.3805e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.1280e-01],\n",
      "        [-1.3868e-01],\n",
      "        [-3.2526e-02],\n",
      "        [-2.6977e-02],\n",
      "        [-5.8690e-02],\n",
      "        [-9.1651e-02],\n",
      "        [-7.7734e-03],\n",
      "        [-4.7434e-02],\n",
      "        [-6.5622e-02],\n",
      "        [-1.9913e-02],\n",
      "        [-2.4418e-03],\n",
      "        [ 3.0477e-02],\n",
      "        [-3.0278e-02],\n",
      "        [-9.1433e-02],\n",
      "        [-1.1922e-01],\n",
      "        [-4.4890e-02],\n",
      "        [-1.6911e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [-1.5846e-01],\n",
      "        [-1.4416e-01],\n",
      "        [-1.4545e-01],\n",
      "        [-1.2121e-01],\n",
      "        [-1.2845e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [-1.6888e-01],\n",
      "        [-6.5827e-02],\n",
      "        [-2.8822e-02],\n",
      "        [ 1.2965e-02],\n",
      "        [ 6.6108e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.1494e-01],\n",
      "        [-9.5307e-02],\n",
      "        [-1.2610e-01],\n",
      "        [-1.8965e-02],\n",
      "        [-3.9558e-02],\n",
      "        [ 1.4232e-03],\n",
      "        [-9.1694e-02],\n",
      "        [-5.4690e-02],\n",
      "        [-2.3654e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [ 3.0995e-02],\n",
      "        [ 8.0944e-03],\n",
      "        [-5.0464e-02],\n",
      "        [-5.4171e-02],\n",
      "        [ 4.4668e-03],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.2558e-01],\n",
      "        [-1.0036e-01],\n",
      "        [-5.0280e-02],\n",
      "        [-1.3144e-01],\n",
      "        [-1.2242e-01],\n",
      "        [-5.9963e-02],\n",
      "        [-2.7400e-02],\n",
      "        [-4.6379e-03],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-6.6575e-02],\n",
      "        [-7.5452e-02],\n",
      "        [-1.3137e-01],\n",
      "        [-1.9247e-02],\n",
      "        [-3.0115e-02],\n",
      "        [-7.5794e-02],\n",
      "        [-3.8160e-02],\n",
      "        [-5.0294e-02],\n",
      "        [-3.3481e-02],\n",
      "        [ 2.1513e-02],\n",
      "        [ 3.0321e-02],\n",
      "        [-1.4909e-02],\n",
      "        [-1.7977e-01],\n",
      "        [-2.4517e-02],\n",
      "        [-9.7523e-02],\n",
      "        [-1.0471e-01],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [-1.4468e-01],\n",
      "        [-1.0871e-01],\n",
      "        [-8.8688e-02],\n",
      "        [-1.3087e-02],\n",
      "        [-8.2648e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.3668e-01],\n",
      "        [-1.7781e-01],\n",
      "        [-1.3870e-01],\n",
      "        [-2.2090e-02],\n",
      "        [-3.0753e-02],\n",
      "        [-8.1297e-02],\n",
      "        [-3.8425e-02],\n",
      "        [-1.2302e-01],\n",
      "        [-7.7197e-02],\n",
      "        [-1.0789e-01],\n",
      "        [-1.1786e-01],\n",
      "        [-1.4990e-01],\n",
      "        [-8.8073e-02],\n",
      "        [-1.2004e-01],\n",
      "        [-1.4988e-01],\n",
      "        [ 5.8412e-03],\n",
      "        [-5.1585e-02],\n",
      "        [-1.1664e-01],\n",
      "        [-1.2577e-01],\n",
      "        [-5.0747e-02],\n",
      "        [-6.9807e-02],\n",
      "        [-1.3295e-02],\n",
      "        [ 1.2085e-01],\n",
      "        [ 7.2997e-02],\n",
      "        [-5.0733e-02],\n",
      "        [-4.6560e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-1.1604e-01],\n",
      "        [-6.5111e-02],\n",
      "        [ 6.0763e-02],\n",
      "        [ 6.0042e-02],\n",
      "        [ 6.5318e-02],\n",
      "        [ 7.7386e-02],\n",
      "        [ 7.7011e-02],\n",
      "        [ 3.5070e-02],\n",
      "        [-9.6634e-06],\n",
      "        [-9.0331e-02],\n",
      "        [-1.2709e-01],\n",
      "        [-1.4921e-01],\n",
      "        [-1.6222e-01],\n",
      "        [-1.7005e-01],\n",
      "        [-1.5169e-01],\n",
      "        [-2.2900e-01],\n",
      "        [-7.8041e-03],\n",
      "        [-2.7300e-02],\n",
      "        [-7.5269e-02],\n",
      "        [-9.0331e-02],\n",
      "        [-2.2068e-02],\n",
      "        [-4.6478e-02],\n",
      "        [-1.5686e-01],\n",
      "        [-1.5118e-02],\n",
      "        [ 9.5000e-02],\n",
      "        [ 4.4734e-02],\n",
      "        [-2.7572e-02],\n",
      "        [-1.2437e-01],\n",
      "        [-3.9691e-02]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(in_features=512, out_features=1)\n",
    "fc_out = fc(lstm_out.contiguous().view(-1, 512))\n",
    "print ('FC layer output shape', fc_out.shape)\n",
    "print ('FC layer output ', fc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid layer output shape torch.Size([500, 1])\n",
      "Sigmoid layer output  tensor([[0.4774],\n",
      "        [0.4683],\n",
      "        [0.4700],\n",
      "        [0.4905],\n",
      "        [0.4821],\n",
      "        [0.4909],\n",
      "        [0.4720],\n",
      "        [0.4683],\n",
      "        [0.4594],\n",
      "        [0.4357],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4524],\n",
      "        [0.4737],\n",
      "        [0.4912],\n",
      "        [0.4777],\n",
      "        [0.4796],\n",
      "        [0.4979],\n",
      "        [0.5093],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.4758],\n",
      "        [0.4608],\n",
      "        [0.4978],\n",
      "        [0.4851],\n",
      "        [0.4790],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4663],\n",
      "        [0.4638],\n",
      "        [0.4914],\n",
      "        [0.4939],\n",
      "        [0.4814],\n",
      "        [0.4941],\n",
      "        [0.4774],\n",
      "        [0.4836],\n",
      "        [0.5066],\n",
      "        [0.4950],\n",
      "        [0.5013],\n",
      "        [0.5007],\n",
      "        [0.4941],\n",
      "        [0.5236],\n",
      "        [0.5093],\n",
      "        [0.4958],\n",
      "        [0.4774],\n",
      "        [0.4762],\n",
      "        [0.4839],\n",
      "        [0.5027],\n",
      "        [0.4800],\n",
      "        [0.4762],\n",
      "        [0.4832],\n",
      "        [0.4901],\n",
      "        [0.5036],\n",
      "        [0.5314],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4682],\n",
      "        [0.4801],\n",
      "        [0.4807],\n",
      "        [0.4655],\n",
      "        [0.4941],\n",
      "        [0.4892],\n",
      "        [0.4935],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.4602],\n",
      "        [0.4514],\n",
      "        [0.5015],\n",
      "        [0.5060],\n",
      "        [0.4942],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4721],\n",
      "        [0.4741],\n",
      "        [0.4817],\n",
      "        [0.4965],\n",
      "        [0.4954],\n",
      "        [0.4916],\n",
      "        [0.5202],\n",
      "        [0.4774],\n",
      "        [0.4527],\n",
      "        [0.4492],\n",
      "        [0.4900],\n",
      "        [0.4845],\n",
      "        [0.4722],\n",
      "        [0.4933],\n",
      "        [0.5242],\n",
      "        [0.5063],\n",
      "        [0.5208],\n",
      "        [0.4774],\n",
      "        [0.4750],\n",
      "        [0.4583],\n",
      "        [0.4359],\n",
      "        [0.4657],\n",
      "        [0.4845],\n",
      "        [0.4723],\n",
      "        [0.4731],\n",
      "        [0.4809],\n",
      "        [0.4947],\n",
      "        [0.4774],\n",
      "        [0.4646],\n",
      "        [0.5154],\n",
      "        [0.4867],\n",
      "        [0.4790],\n",
      "        [0.4790],\n",
      "        [0.4986],\n",
      "        [0.5261],\n",
      "        [0.4972],\n",
      "        [0.4882],\n",
      "        [0.4774],\n",
      "        [0.4796],\n",
      "        [0.4640],\n",
      "        [0.4663],\n",
      "        [0.4765],\n",
      "        [0.4925],\n",
      "        [0.4806],\n",
      "        [0.4878],\n",
      "        [0.4868],\n",
      "        [0.5165],\n",
      "        [0.4709],\n",
      "        [0.4429],\n",
      "        [0.4608],\n",
      "        [0.4849],\n",
      "        [0.4693],\n",
      "        [0.4717],\n",
      "        [0.4732],\n",
      "        [0.4630],\n",
      "        [0.4950],\n",
      "        [0.4895],\n",
      "        [0.4774],\n",
      "        [0.4770],\n",
      "        [0.4739],\n",
      "        [0.4809],\n",
      "        [0.5238],\n",
      "        [0.5102],\n",
      "        [0.4969],\n",
      "        [0.5085],\n",
      "        [0.5032],\n",
      "        [0.5119],\n",
      "        [0.5027],\n",
      "        [0.4946],\n",
      "        [0.5165],\n",
      "        [0.4847],\n",
      "        [0.4738],\n",
      "        [0.4842],\n",
      "        [0.5052],\n",
      "        [0.4781],\n",
      "        [0.4944],\n",
      "        [0.4908],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.4585],\n",
      "        [0.4664],\n",
      "        [0.4655],\n",
      "        [0.4331],\n",
      "        [0.4597],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4658],\n",
      "        [0.4582],\n",
      "        [0.4706],\n",
      "        [0.4870],\n",
      "        [0.4799],\n",
      "        [0.5060],\n",
      "        [0.5034],\n",
      "        [0.5131],\n",
      "        [0.4822],\n",
      "        [0.4819],\n",
      "        [0.4891],\n",
      "        [0.5156],\n",
      "        [0.5080],\n",
      "        [0.5137],\n",
      "        [0.5053],\n",
      "        [0.5003],\n",
      "        [0.4978],\n",
      "        [0.5119],\n",
      "        [0.4774],\n",
      "        [0.4694],\n",
      "        [0.4845],\n",
      "        [0.4817],\n",
      "        [0.4992],\n",
      "        [0.4897],\n",
      "        [0.4561],\n",
      "        [0.4538],\n",
      "        [0.4745],\n",
      "        [0.4701],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4952],\n",
      "        [0.4782],\n",
      "        [0.4751],\n",
      "        [0.4866],\n",
      "        [0.4696],\n",
      "        [0.4884],\n",
      "        [0.4960],\n",
      "        [0.5137],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4838],\n",
      "        [0.4724],\n",
      "        [0.4577],\n",
      "        [0.5120],\n",
      "        [0.5091],\n",
      "        [0.4974],\n",
      "        [0.4893],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4745],\n",
      "        [0.4810],\n",
      "        [0.5030],\n",
      "        [0.4784],\n",
      "        [0.4726],\n",
      "        [0.5070],\n",
      "        [0.5191],\n",
      "        [0.4967],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4739],\n",
      "        [0.4538],\n",
      "        [0.4655],\n",
      "        [0.4418],\n",
      "        [0.4487],\n",
      "        [0.4653],\n",
      "        [0.4528],\n",
      "        [0.4546],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4931],\n",
      "        [0.4980],\n",
      "        [0.5026],\n",
      "        [0.4934],\n",
      "        [0.4958],\n",
      "        [0.4873],\n",
      "        [0.5022],\n",
      "        [0.4904],\n",
      "        [0.4724],\n",
      "        [0.4979],\n",
      "        [0.4571],\n",
      "        [0.4827],\n",
      "        [0.4966],\n",
      "        [0.4886],\n",
      "        [0.4967],\n",
      "        [0.4952],\n",
      "        [0.4900],\n",
      "        [0.5142],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4643],\n",
      "        [0.4772],\n",
      "        [0.4654],\n",
      "        [0.4916],\n",
      "        [0.4916],\n",
      "        [0.4662],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.4564],\n",
      "        [0.4556],\n",
      "        [0.4372],\n",
      "        [0.4533],\n",
      "        [0.4414],\n",
      "        [0.5094],\n",
      "        [0.4967],\n",
      "        [0.4532],\n",
      "        [0.4845],\n",
      "        [0.4866],\n",
      "        [0.4870],\n",
      "        [0.4994],\n",
      "        [0.4953],\n",
      "        [0.5085],\n",
      "        [0.4714],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4673],\n",
      "        [0.4836],\n",
      "        [0.4830],\n",
      "        [0.4755],\n",
      "        [0.4834],\n",
      "        [0.4936],\n",
      "        [0.4827],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.4330],\n",
      "        [0.4644],\n",
      "        [0.4624],\n",
      "        [0.4631],\n",
      "        [0.4602],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4725],\n",
      "        [0.4654],\n",
      "        [0.4750],\n",
      "        [0.5151],\n",
      "        [0.5204],\n",
      "        [0.5151],\n",
      "        [0.5173],\n",
      "        [0.4774],\n",
      "        [0.4994],\n",
      "        [0.4968],\n",
      "        [0.5033],\n",
      "        [0.4801],\n",
      "        [0.4958],\n",
      "        [0.4887],\n",
      "        [0.4948],\n",
      "        [0.4930],\n",
      "        [0.4805],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.4564],\n",
      "        [0.4818],\n",
      "        [0.5020],\n",
      "        [0.4923],\n",
      "        [0.4655],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4718],\n",
      "        [0.4654],\n",
      "        [0.4919],\n",
      "        [0.4933],\n",
      "        [0.4853],\n",
      "        [0.4771],\n",
      "        [0.4981],\n",
      "        [0.4881],\n",
      "        [0.4836],\n",
      "        [0.4950],\n",
      "        [0.4994],\n",
      "        [0.5076],\n",
      "        [0.4924],\n",
      "        [0.4772],\n",
      "        [0.4702],\n",
      "        [0.4888],\n",
      "        [0.4578],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.4605],\n",
      "        [0.4640],\n",
      "        [0.4637],\n",
      "        [0.4697],\n",
      "        [0.4679],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.4579],\n",
      "        [0.4835],\n",
      "        [0.4928],\n",
      "        [0.5032],\n",
      "        [0.5165],\n",
      "        [0.4774],\n",
      "        [0.4713],\n",
      "        [0.4762],\n",
      "        [0.4685],\n",
      "        [0.4953],\n",
      "        [0.4901],\n",
      "        [0.5004],\n",
      "        [0.4771],\n",
      "        [0.4863],\n",
      "        [0.4411],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.5077],\n",
      "        [0.5020],\n",
      "        [0.4874],\n",
      "        [0.4865],\n",
      "        [0.5011],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4686],\n",
      "        [0.4749],\n",
      "        [0.4874],\n",
      "        [0.4672],\n",
      "        [0.4694],\n",
      "        [0.4850],\n",
      "        [0.4932],\n",
      "        [0.4988],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4834],\n",
      "        [0.4811],\n",
      "        [0.4672],\n",
      "        [0.4952],\n",
      "        [0.4925],\n",
      "        [0.4811],\n",
      "        [0.4905],\n",
      "        [0.4874],\n",
      "        [0.4916],\n",
      "        [0.5054],\n",
      "        [0.5076],\n",
      "        [0.4963],\n",
      "        [0.4552],\n",
      "        [0.4939],\n",
      "        [0.4756],\n",
      "        [0.4738],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.4639],\n",
      "        [0.4728],\n",
      "        [0.4778],\n",
      "        [0.4967],\n",
      "        [0.4793],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4659],\n",
      "        [0.4557],\n",
      "        [0.4654],\n",
      "        [0.4945],\n",
      "        [0.4923],\n",
      "        [0.4797],\n",
      "        [0.4904],\n",
      "        [0.4693],\n",
      "        [0.4807],\n",
      "        [0.4731],\n",
      "        [0.4706],\n",
      "        [0.4626],\n",
      "        [0.4780],\n",
      "        [0.4700],\n",
      "        [0.4626],\n",
      "        [0.5015],\n",
      "        [0.4871],\n",
      "        [0.4709],\n",
      "        [0.4686],\n",
      "        [0.4873],\n",
      "        [0.4826],\n",
      "        [0.4967],\n",
      "        [0.5302],\n",
      "        [0.5182],\n",
      "        [0.4873],\n",
      "        [0.4884],\n",
      "        [0.4774],\n",
      "        [0.4710],\n",
      "        [0.4837],\n",
      "        [0.5152],\n",
      "        [0.5150],\n",
      "        [0.5163],\n",
      "        [0.5193],\n",
      "        [0.5192],\n",
      "        [0.5088],\n",
      "        [0.5000],\n",
      "        [0.4774],\n",
      "        [0.4683],\n",
      "        [0.4628],\n",
      "        [0.4595],\n",
      "        [0.4576],\n",
      "        [0.4621],\n",
      "        [0.4430],\n",
      "        [0.4980],\n",
      "        [0.4932],\n",
      "        [0.4812],\n",
      "        [0.4774],\n",
      "        [0.4945],\n",
      "        [0.4884],\n",
      "        [0.4609],\n",
      "        [0.4962],\n",
      "        [0.5237],\n",
      "        [0.5112],\n",
      "        [0.4931],\n",
      "        [0.4689],\n",
      "        [0.4901]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "sigm = nn.Sigmoid()\n",
    "sigm_out = sigm(fc_out)\n",
    "print ('Sigmoid layer output shape', sigm_out.shape)\n",
    "print ('Sigmoid layer output ', sigm_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer output shape torch.Size([50, 10])\n",
      "Output layer output  tensor([[0.4774, 0.4683, 0.4700, 0.4905, 0.4821, 0.4909, 0.4720, 0.4683, 0.4594,\n",
      "         0.4357],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4524, 0.4737, 0.4912, 0.4777, 0.4796, 0.4979,\n",
      "         0.5093],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.4758, 0.4608, 0.4978, 0.4851,\n",
      "         0.4790],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4663, 0.4638, 0.4914, 0.4939, 0.4814,\n",
      "         0.4941],\n",
      "        [0.4774, 0.4836, 0.5066, 0.4950, 0.5013, 0.5007, 0.4941, 0.5236, 0.5093,\n",
      "         0.4958],\n",
      "        [0.4774, 0.4762, 0.4839, 0.5027, 0.4800, 0.4762, 0.4832, 0.4901, 0.5036,\n",
      "         0.5314],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4682, 0.4801, 0.4807, 0.4655, 0.4941, 0.4892,\n",
      "         0.4935],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.4602, 0.4514, 0.5015, 0.5060,\n",
      "         0.4942],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4721, 0.4741, 0.4817, 0.4965, 0.4954, 0.4916,\n",
      "         0.5202],\n",
      "        [0.4774, 0.4527, 0.4492, 0.4900, 0.4845, 0.4722, 0.4933, 0.5242, 0.5063,\n",
      "         0.5208],\n",
      "        [0.4774, 0.4750, 0.4583, 0.4359, 0.4657, 0.4845, 0.4723, 0.4731, 0.4809,\n",
      "         0.4947],\n",
      "        [0.4774, 0.4646, 0.5154, 0.4867, 0.4790, 0.4790, 0.4986, 0.5261, 0.4972,\n",
      "         0.4882],\n",
      "        [0.4774, 0.4796, 0.4640, 0.4663, 0.4765, 0.4925, 0.4806, 0.4878, 0.4868,\n",
      "         0.5165],\n",
      "        [0.4709, 0.4429, 0.4608, 0.4849, 0.4693, 0.4717, 0.4732, 0.4630, 0.4950,\n",
      "         0.4895],\n",
      "        [0.4774, 0.4770, 0.4739, 0.4809, 0.5238, 0.5102, 0.4969, 0.5085, 0.5032,\n",
      "         0.5119],\n",
      "        [0.5027, 0.4946, 0.5165, 0.4847, 0.4738, 0.4842, 0.5052, 0.4781, 0.4944,\n",
      "         0.4908],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.4585, 0.4664, 0.4655, 0.4331,\n",
      "         0.4597],\n",
      "        [0.4774, 0.4683, 0.4658, 0.4582, 0.4706, 0.4870, 0.4799, 0.5060, 0.5034,\n",
      "         0.5131],\n",
      "        [0.4822, 0.4819, 0.4891, 0.5156, 0.5080, 0.5137, 0.5053, 0.5003, 0.4978,\n",
      "         0.5119],\n",
      "        [0.4774, 0.4694, 0.4845, 0.4817, 0.4992, 0.4897, 0.4561, 0.4538, 0.4745,\n",
      "         0.4701],\n",
      "        [0.4774, 0.4683, 0.4952, 0.4782, 0.4751, 0.4866, 0.4696, 0.4884, 0.4960,\n",
      "         0.5137],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4838, 0.4724, 0.4577, 0.5120, 0.5091, 0.4974,\n",
      "         0.4893],\n",
      "        [0.4774, 0.4683, 0.4745, 0.4810, 0.5030, 0.4784, 0.4726, 0.5070, 0.5191,\n",
      "         0.4967],\n",
      "        [0.4774, 0.4683, 0.4739, 0.4538, 0.4655, 0.4418, 0.4487, 0.4653, 0.4528,\n",
      "         0.4546],\n",
      "        [0.4774, 0.4683, 0.4931, 0.4980, 0.5026, 0.4934, 0.4958, 0.4873, 0.5022,\n",
      "         0.4904],\n",
      "        [0.4724, 0.4979, 0.4571, 0.4827, 0.4966, 0.4886, 0.4967, 0.4952, 0.4900,\n",
      "         0.5142],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4643, 0.4772, 0.4654, 0.4916, 0.4916,\n",
      "         0.4662],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.4564, 0.4556, 0.4372, 0.4533,\n",
      "         0.4414],\n",
      "        [0.5094, 0.4967, 0.4532, 0.4845, 0.4866, 0.4870, 0.4994, 0.4953, 0.5085,\n",
      "         0.4714],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4673, 0.4836, 0.4830, 0.4755, 0.4834, 0.4936,\n",
      "         0.4827],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.4330, 0.4644, 0.4624, 0.4631,\n",
      "         0.4602],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4725, 0.4654, 0.4750, 0.5151, 0.5204, 0.5151,\n",
      "         0.5173],\n",
      "        [0.4774, 0.4994, 0.4968, 0.5033, 0.4801, 0.4958, 0.4887, 0.4948, 0.4930,\n",
      "         0.4805],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.4564, 0.4818, 0.5020, 0.4923,\n",
      "         0.4655],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4718, 0.4654, 0.4919, 0.4933, 0.4853, 0.4771,\n",
      "         0.4981],\n",
      "        [0.4881, 0.4836, 0.4950, 0.4994, 0.5076, 0.4924, 0.4772, 0.4702, 0.4888,\n",
      "         0.4578],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.4605, 0.4640, 0.4637, 0.4697,\n",
      "         0.4679],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.4579, 0.4835, 0.4928, 0.5032,\n",
      "         0.5165],\n",
      "        [0.4774, 0.4713, 0.4762, 0.4685, 0.4953, 0.4901, 0.5004, 0.4771, 0.4863,\n",
      "         0.4411],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.5077, 0.5020, 0.4874, 0.4865,\n",
      "         0.5011],\n",
      "        [0.4774, 0.4683, 0.4686, 0.4749, 0.4874, 0.4672, 0.4694, 0.4850, 0.4932,\n",
      "         0.4988],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4834, 0.4811, 0.4672, 0.4952, 0.4925,\n",
      "         0.4811],\n",
      "        [0.4905, 0.4874, 0.4916, 0.5054, 0.5076, 0.4963, 0.4552, 0.4939, 0.4756,\n",
      "         0.4738],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.4639, 0.4728, 0.4778, 0.4967,\n",
      "         0.4793],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4659, 0.4557, 0.4654, 0.4945, 0.4923,\n",
      "         0.4797],\n",
      "        [0.4904, 0.4693, 0.4807, 0.4731, 0.4706, 0.4626, 0.4780, 0.4700, 0.4626,\n",
      "         0.5015],\n",
      "        [0.4871, 0.4709, 0.4686, 0.4873, 0.4826, 0.4967, 0.5302, 0.5182, 0.4873,\n",
      "         0.4884],\n",
      "        [0.4774, 0.4710, 0.4837, 0.5152, 0.5150, 0.5163, 0.5193, 0.5192, 0.5088,\n",
      "         0.5000],\n",
      "        [0.4774, 0.4683, 0.4628, 0.4595, 0.4576, 0.4621, 0.4430, 0.4980, 0.4932,\n",
      "         0.4812],\n",
      "        [0.4774, 0.4945, 0.4884, 0.4609, 0.4962, 0.5237, 0.5112, 0.4931, 0.4689,\n",
      "         0.4901]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = x.shape[0]\n",
    "out = sigm_out.view(batch_size, -1)\n",
    "print ('Output layer output shape', out.shape)\n",
    "print ('Output layer output ', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
